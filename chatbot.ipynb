{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1971ea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "Chatbot: hello .\n",
      "You: how are you\n",
      "Chatbot: i am fine .\n",
      "You: what is your name\n",
      "Chatbot: my name is chatbot .\n",
      "You: what is your age\n",
      "Chatbot: i am robo , i donot have age .\n",
      "You: what is your use\n",
      "Chatbot: i am robot .\n",
      "You: what will you do\n",
      "Chatbot: i will assist you .\n",
      "You: thank you\n",
      "Chatbot: have a great day .\n",
      "You: how is life going on\n",
      "Chatbot: I'm sorry, I didn't understand.\n",
      "You: how will you help me\n",
      "Chatbot: I'm sorry, I didn't understand.\n",
      "You: exit\n",
      "Chatbot: Bye.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Function to load and preprocess your dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    with io.open(file_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_sentences(corpus):\n",
    "    sentences = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [[lemmatizer.lemmatize(word.lower()) for word in sentence] for sentence in tokens]\n",
    "    \n",
    "    return lemmatized_tokens, lemmatizer\n",
    "\n",
    "\n",
    "# Function to create TF-IDF matrix\n",
    "def create_tfidf_matrix(lemmatized_corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens) for tokens in lemmatized_corpus])\n",
    "    \n",
    "    return vectorizer, tfidf_matrix, lemmatized_corpus\n",
    "\n",
    "\n",
    "# Function to save processed data using pickle\n",
    "def save_processed_data(data, file_path='processed_data.pkl'):\n",
    "    \n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "        \n",
    "# Function for chatbot response\n",
    "def chatbot_response(user_tfidf, X_train_tfidf, lemmatized_corpus, vectorizer, user_input, threshold=0.2):\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    cosine_similarities = cosine_similarity(user_tfidf, X_train_tfidf).flatten()\n",
    "    most_similar_index = np.argmax(cosine_similarities)\n",
    "    highest_similarity = cosine_similarities[most_similar_index]\n",
    "\n",
    "    # Check if the highest similarity is below the threshold\n",
    "    if highest_similarity < threshold:\n",
    "        return \"I'm sorry, I didn't understand.\"\n",
    "\n",
    "    # Check if the lowercased user input is a substring of the lowercased response tokens\n",
    "    if user_input.lower() not in ' '.join(lemmatized_corpus[most_similar_index]).lower():\n",
    "        return \"I'm sorry, I didn't understand.\"\n",
    "\n",
    "    # If the conditions are not met, proceed to generate the response\n",
    "    response_tokens = lemmatized_corpus[most_similar_index]\n",
    "\n",
    "    # Extract relevant information based on user's question\n",
    "    extracted_info = extract_information(user_input, response_tokens)\n",
    "\n",
    "    # If no information is extracted, use the whole response\n",
    "    if not extracted_info:\n",
    "        extracted_info = response_tokens\n",
    "\n",
    "    chatbot_response_str = ' '.join(extracted_info).strip()\n",
    "\n",
    "    # Remove the user's question part from the response\n",
    "    chatbot_response_str = chatbot_response_str.lstrip(user_input.lower())\n",
    "    \n",
    "    # Remove leading punctuation including a comma\n",
    "    chatbot_response_str = chatbot_response_str.lstrip(string.punctuation).strip()\n",
    "\n",
    "    # Remove leading comma if present\n",
    "    if chatbot_response_str.startswith(\",\"):\n",
    "        chatbot_response_str = chatbot_response_str[1:].strip()\n",
    "\n",
    "    return chatbot_response_str\n",
    "\n",
    "\n",
    "# Function to extract information from user input\n",
    "def extract_information(user_input, response_tokens):\n",
    "    \n",
    "    lemmatized_user_input = lemmatize_tokens([nltk.word_tokenize(user_input)])[0]\n",
    "\n",
    "    extracted_info = []\n",
    "\n",
    "    # Extract relevant information based on user's question\n",
    "    if any(word in lemmatized_user_input for word in ['name', 'called']):\n",
    "        extracted_info = [word for word in response_tokens if word.lower() not in ['my', 'name', 'is', 'called']]\n",
    "    elif any(word in lemmatized_user_input for word in ['age']):\n",
    "        extracted_info = [word for word in response_tokens if word.lower() not in ['my', 'age', 'is']]\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "# File path for the dataset\n",
    "file_path = './data/basic_details.txt'\n",
    "corpus = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "tokenized_sentences = tokenize_sentences(corpus)\n",
    "lemmatized_tokens, lemmatizer = lemmatize_tokens(tokenized_sentences)\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "vectorizer, tfidf_matrix, lemmatized_corpus = create_tfidf_matrix(lemmatized_tokens)\n",
    "\n",
    "# Save the processed data and model objects using pickle\n",
    "save_processed_data(tfidf_matrix, file_path='processed_data.pkl')\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Add a condition to exit the loop if the user wants to end the conversation\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Bye.\")\n",
    "        break\n",
    "\n",
    "    user_input_tokens = nltk.word_tokenize(user_input)\n",
    "    user_input_lemmatized = lemmatize_tokens([user_input_tokens])[0]\n",
    "    user_input_lemmatized_sentence = ' '.join(' '.join(sentence) for sentence in user_input_lemmatized)\n",
    "\n",
    "    user_tfidf = vectorizer.transform([user_input_lemmatized_sentence])\n",
    "\n",
    "    # Chatbot response\n",
    "    response = chatbot_response(user_tfidf, tfidf_matrix, lemmatized_corpus, vectorizer, user_input)\n",
    "\n",
    "    # Print each sentence in the chatbot's response separately\n",
    "    for sentence in response.split('. '):\n",
    "        print(\"Chatbot:\", sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2cd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
